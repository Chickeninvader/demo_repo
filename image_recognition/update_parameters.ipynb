{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate, L, iteration, parameters_adam, beta_1=0.9, beta_2=0.9):\n",
        "    \"\"\"\n",
        "    Updates parameters using the gradient descent update rule given above using Adam gradient descent \n",
        "    algorithm\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients \n",
        "    learning_rate -- the \"machine learning's speed\"\n",
        "    L -- number of hidden layers\n",
        "    beta_1 -- momentum \n",
        "    beta_2 -- RMS prop\n",
        "    iteration -- index of for loop\n",
        "    parameters_adam -- parameters used for gradient descent algorithm\n",
        "\n",
        "    Returns:\n",
        "    parameters_update, parameters_adam_update -- python dictionary containing your updated parameters \n",
        "    \"\"\"\n",
        "    # initialize\n",
        "    W = {}\n",
        "    b = {}\n",
        "    dW = {}\n",
        "    db = {}\n",
        "    v_dW = {} \n",
        "    v_db = {}\n",
        "    s_dW = {}\n",
        "    s_db = {}\n",
        "    epsilon = 10 ** -8\n",
        "    parameters_update = {}\n",
        "    parameters_adam_update = {}\n",
        "\n",
        "    for i in range(1, L+1):\n",
        "\n",
        "        # Take parameters from dictionary\n",
        "        W[i] = parameters[\"W\" + str(i)]\n",
        "        b[i] = parameters[\"b\" + str(i)]\n",
        "        dW[i] = grads[\"dW\" + str(i)]\n",
        "        db[i] = grads[\"db\" + str(i)]\n",
        "        v_dW[i] = parameters_adam['v_dW' + str(i)]\n",
        "        v_db[i] = parameters_adam['v_db' + str(i)]\n",
        "        #s_dW[i] = parameters_adam['s_dW' + str(i)]\n",
        "        #s_db[i] = parameters_adam['s_db' + str(i)]\n",
        "\n",
        "    for i in range(1, L+1):\n",
        "\n",
        "        # Update parameters\n",
        "        v_dW[i] = (beta_1 * v_dW[i] + (1 - beta_1) * dW[i]) # / (1 - beta_1**iteration)\n",
        "        v_db[i] = (beta_1 * v_db[i] + (1 - beta_1) * db[i]) # / (1 - beta_1**iteration)\n",
        "        #s_dW[i] = (beta_2 * s_dW[i] + (1 - beta_2) * np.power(dW[i], 2)) / (1 - beta_2**iteration)\n",
        "        #s_db[i] = (beta_2 * s_db[i] + (1 - beta_2) * np.power(db[i], 2)) / (1 - beta_2**iteration)\n",
        "\n",
        "        W[i] = W[i] - learning_rate*v_dW[i] #/ (np.sqrt(s_dW[i]) + epsilon)\n",
        "        b[i] = b[i] - learning_rate*v_db[i] #/ (np.sqrt(s_db[i]) + epsilon)\n",
        "\n",
        "        # Save parameters\n",
        "        parameters_adam_update[\"v_dW\" + str(i)] = v_dW[i]\n",
        "        parameters_adam_update[\"v_db\" + str(i)] = v_db[i]\n",
        "        #parameters_adam_update[\"s_dW\" + str(i)] = s_dW[i]\n",
        "        #parameters_adam_update[\"s_db\" + str(i)] = s_db[i]\n",
        "        parameters_update[\"W\" + str(i)] = W[i]\n",
        "        parameters_update[\"b\" + str(i)] = b[i]\n",
        "\n",
        "    return parameters_update, parameters_adam_update\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}